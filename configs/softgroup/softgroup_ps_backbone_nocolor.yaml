model:
  channels: 16  # number of base channel for the backbone network
  num_blocks: 7 # number of backbone blocks
  semantic_classes: 2 # stem leaf
  instance_classes: 1 # leaf
  sem2ins_classes: [0]  # stem, class index to get instance directly from semantic. 
  semantic_only: True #  Set it to True using for pretraining the backbone. 
  ignore_label: -100
  semantic_weight: [0.187, 0.813]
  # with_coords: False
  use_color: False

  grouping_cfg:
    score_thr: 0.2
    radius: 0.1
    mean_active: 300
    class_numpoint_mean: [-1., 344.]  # stem不进行instance segmentation
    npoint_thr: 0.05  # absolute if class_numpoint == -1, relative if class_numpoint != -1
    ignore_classes: [0]

  instance_voxel_cfg:
    scale: 50  # scaling factor, voxel size = 1 / scale. In this case voxel_size = 1/50 = 0.02m
    spatial_shape: 20 # the dimension of instance in terms of voxels, i.e., H, W, D of instance will be 20 voxels.

  train_cfg:
    max_proposal_num: 400
    pos_iou_thr: 0.5

  test_cfg:
    x4_split: False
    cls_score_thr: 0.001
    mask_score_thr: -0.5
    min_npoint: 20
    eval_tasks: ['semantic']
  fixed_modules: []

data:
  train:
    type: 'ps'
    data_root: 'dataset/ps/preprocess_split'
    prefix: 'train'
    suffix: '_inst_nostuff.pth'
    training: True
    repeat: 8
    voxel_cfg:
      scale: 50
      spatial_shape: [128, 512]
      max_npoint: 250000
      min_npoint: 4000
  test:
    type: 'ps'
    data_root: 'dataset/ps/preprocess_split'
    prefix: 'test'
    suffix: '_inst_nostuff.pth'
    training: False
    voxel_cfg:
      scale: 50
      spatial_shape: [128, 512]
      max_npoint: 250000
      min_npoint: 4000

dataloader:
  train:
    batch_size: 5
    num_workers: 12
  test:
    batch_size: 1
    num_workers: 1

optimizer:
  type: 'Adam'
  lr: 0.004

save_cfg:
  semantic: True
  offset: True
  instance: True

fp16: False
epochs: 50
step_epoch: 20
save_freq: 10
pretrain: './checkpoint/hais_ckpt_spconv2.pth'
work_dir: 'work_dirs/softgroup_ps_backbone_nocolor_trial1'
